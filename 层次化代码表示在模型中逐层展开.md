+ 对注意力分布的分析发现，在模型底层的注意力分布相对平均，而随着层数增高，注意力分布则变得更sharp。因此对于长上下文计算来说，高层可以使用更高的kv压缩率，模型底层的注意力计算相比模型高层需要花费更多资源
+ 另一方面，代码项目上下文具有层次化结构，例如可以组织为文件夹级-文件级-类级-函数级-行级-token级，更抽象的层次（文件夹级、文件级）对应着更少的kv对



基于此，设计一种在模型不同层对上下文采用不同抽象级别表示的decoding方法。在一个生成步内：

+ 当计算处在模型底层时，难以压缩kv cache。因此使用更抽象的表示，例如文件夹表示对应的kv序列，就算保留全部kv cache也不需要长上下文窗口
+ 而当不断计算到模型高层时，则可以将抽象层次表示逐渐细化，例如token序列对应的kv序列，此时虽然token序列本身很长，但由于可以对kv cache进行eviction或merge来压缩，因此也不需要长上下文窗口
+ 至于选择哪些抽象表示展开，则可以基于当前层在各个抽象表示上的注意力分布。例如当前层是文件级表示，列出了一系列文件名对应的kv cache；而当前query对第二个文件的关注很高，对其他的都不关注，则可以在下一层列出第二个文件内各个类对应的kv cache，而其他文件表示则被eviction
+ 在不同生成步之间可以引入缓存机制，记录之前的生成步模型关注了哪些kv，则缓存下来防止重复计算

![画板](https://intranetproxy.alipay.com/skylark/lark/0/2024/jpeg/141056448/1722273361224-8987ab7e-24e3-49cf-ad26-f9e780f2eca1.jpeg)



不过，模型本身并未在这种任务上进行训练，应该是需要微调



对比基于token的RAG方法，不需要引入额外的检索器，完全基于模型本身的能力和偏好

对比基于kv cache的RAG方法，不需要提前计算全部上下文的kv对，而可以在生成过程中随用随算

此外，本方法实现了一种先抽象再具象的层次化检索，能让模型在逐层计算的过程中逐渐深入所需的上下文，并动态地决定需要深入展开多少上下文，符合人类的阅读习惯？



